{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from attack.fast_gradient_method_preprocess import fast_gradient_method\n",
    "from attack.projected_gradient_descent_preprocess import projected_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/nthudatalab1/Jimmy/RMC/ICML_377_runtime_masking_cleansing/ImageNet'\n",
    "TRAIN_DATA_DIR = '/data/train_path.npy'\n",
    "TRAIN_LABEL_DIR = '/data/train_label.npy'\n",
    "EVAL_DATA_DIR = '/data/eval_path.npy'\n",
    "EVAL_LABEL_DIR = '/data/eval_label.npy'\n",
    "ATTACK_DATA_DIR = ''\n",
    "ATTACK_LABEL_DIR = ''\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "IMG_SIZE = 224\n",
    "RESIZE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "# Training data\n",
    "train_path = np.load(BASE_DIR + TRAIN_DATA_DIR)\n",
    "train_label = np.load(BASE_DIR + TRAIN_LABEL_DIR)\n",
    "assert len(train_path) == len(train_label)\n",
    "\n",
    "# Evaluation data\n",
    "eval_path = np.load(BASE_DIR + EVAL_DATA_DIR)\n",
    "eval_label = np.load(BASE_DIR + EVAL_LABEL_DIR)\n",
    "assert len(eval_path) == len(eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _testing_data_generator(image_path, label):\n",
    "    # Read image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = scale19(img)\n",
    "    return img, label\n",
    "\n",
    "def _attacking_data_generator(image_path, label):\n",
    "    # Read image\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "def testing_dataset_generator(img_path, label, dataset_generator, batch_size):\n",
    "    assert img_path.shape[0] == label.shape[0]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_path, label))\n",
    "    dataset = dataset.map(dataset_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, shape=(224,224)):\n",
    "    target_width = shape[0]\n",
    "    target_height = shape[1]\n",
    "    initial_width = tf.shape(image)[0]\n",
    "    initial_height = tf.shape(image)[1]\n",
    "    im = image\n",
    "    ratio = 0\n",
    "    if(initial_width < initial_height):\n",
    "        ratio = tf.cast(256 / initial_width, tf.float32)\n",
    "        h = tf.cast(initial_height, tf.float32) * ratio\n",
    "        im = tf.image.resize(im, (256, h), method=\"bicubic\")\n",
    "    else:\n",
    "        ratio = tf.cast(256 / initial_height, tf.float32)\n",
    "        w = tf.cast(initial_width, tf.float32) * ratio\n",
    "        im = tf.image.resize(im, (w, 256), method=\"bicubic\")\n",
    "    width = tf.shape(im)[0]\n",
    "    height = tf.shape(im)[1]\n",
    "    startx = width//2 - (target_width//2)\n",
    "    starty = height//2 - (target_height//2)\n",
    "    im = tf.image.crop_to_bounding_box(im, startx, starty, target_width, target_height)\n",
    "    return im\n",
    "\n",
    "def scale19(image):\n",
    "    i = resize_image(image, (224,224))\n",
    "    return (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the image size is 224*224, use \"_attacking_data_generator\" as dataset_generator in dataset API\n",
    "# Else use \"_testing_data_generator\" to crop the image first\n",
    "eval_ds = testing_dataset_generator(eval_path, eval_label, _testing_data_generator, BATCH_SIZE)\n",
    "# eval_ds = testing_dataset_generator(eval_path, eval_label, _attacking_data_generator, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet152V2():\n",
    "    IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    resnet152v2 = tf.keras.applications.ResNet152V2(include_top=True,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=IMG_SHAPE, \n",
    "                                                    classes=1000)\n",
    "    return resnet152v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152v2 = ResNet152V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layers(layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    resnet152v2 = tf.keras.applications.ResNet152V2(include_top=True,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=IMG_SHAPE, \n",
    "                                                    classes=1000)\n",
    "    output = [resnet152v2.get_layer(name).output for name in layer_names]\n",
    "    model = tf.keras.Model([resnet152v2.input], output)\n",
    "    return model\n",
    "\n",
    "class ResNet152V2_extractor(tf.keras.Model):\n",
    "    def __init__(self, layers, trainable=False):\n",
    "        super(ResNet152V2_extractor, self).__init__()\n",
    "        self.resnet152v2 =  resnet_layers(layers)\n",
    "        self.resnet152v2.trainable = trainable\n",
    "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    # return the feature map of required layer\n",
    "    def call(self, inputs):\n",
    "        outputs = self.resnet152v2(inputs)\n",
    "        outputs = self.gap(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this paper, we extract features from convolutional layer 5_1\n",
    "# It is possible to extract from different hidden layers\n",
    "resnet152v2_extractor = ResNet152V2_extractor(layers=['conv5_block1_preact_bn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def extract_step(images):\n",
    "    outputs = resnet152v2_extractor(images)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation dataset\n",
    "eval_pgd_feature_list = list()\n",
    "for adv_images, _ in eval_ds:\n",
    "    adv_images = tf.keras.applications.resnet_v2.preprocess_input(adv_images*255)\n",
    "    outputs = extract_step(adv_images)\n",
    "    eval_pgd_feature_list.append(outputs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pgd_feature_list = np.asarray(eval_pgd_feature_list)\n",
    "eval_pgd_feature_list = np.vstack(eval_pgd_feature_list)\n",
    "eval_pgd_feature_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adv_repre, adv_path in zip(eval_pgd_feature_list, eval_path):\n",
    "    np.save(adv_path.replace('val_set', ATTACK_DATA_DIR).replace('JPEG', 'npy'), adv_repre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ATTACK_DATA_DIR+'.npy', eval_pgd_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
